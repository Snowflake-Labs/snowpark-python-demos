{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vanilla-nancy",
   "metadata": {},
   "source": [
    "# Machine Learning with Snowpark Python and Java UDFs\n",
    "\n",
    "\n",
    "## Using rental listings we would like to train a ML model to estimate the rent price of our sale listing : \n",
    "\n",
    "![title](img/BusinessUseCase.png)\n",
    "\n",
    "## We will use the following architecture to prepare our data, training our XGBoost linear regression model and run our model on Snowflake :\n",
    "\n",
    "![title](img/TechUseCase.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-library",
   "metadata": {},
   "source": [
    "# Now to the fun stuff !\n",
    "### Imports the Snowpark library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import *\n",
    "from snowflake.snowpark.functions import *\n",
    "from snowflake.snowpark.types import *\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-genealogy",
   "metadata": {},
   "source": [
    "### Initialising our connection to Snowflake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Creds.json') as f:\n",
    "    connection_parameters = json.load(f)    \n",
    "\n",
    "mySnowSess = Session.builder.configs(connection_parameters).create()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-procedure",
   "metadata": {},
   "source": [
    "# Data Preparation using Data Frames\n",
    "We have Real Estate listings and their postcodes, what we want to do is to remove outliers. For this we create a dataframe which calculates the median.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADS_df_Raw = mySnowSess.table(\"ADS\").filter(col(\"ADS_CATEGORY_NAME\") ==  \"Locations\" )\n",
    "ADS_df_GrpbyCodeInsee_MedianPrice = ADS_df_Raw.select(\n",
    "  col(\"ADS_GEO_ZIPCODE\")\n",
    ", col(\"ADS_GEO_CITY\")\n",
    ", col(\"ADS_ATTR_REAL_ESTATE_TYPE\").as_(\"ADS_ATTR_REAL_ESTATE_TYPE_MEDIAN\")\n",
    ", col(\"ADS_ID\")\n",
    ", (col(\"ADS_PRICE\") / col(\"ADS_ATTR_SQUARE\")).as_(\"ADS_PRICE_SQUARE\")).\\\n",
    "  groupBy(col(\"ADS_GEO_ZIPCODE\"),col(\"ADS_GEO_CITY\"),col(\"ADS_ATTR_REAL_ESTATE_TYPE_MEDIAN\")).\\\n",
    "  agg([count (col(\"ADS_ID\")).as_(\"COUNT_ADS\"), median(col(\"ADS_PRICE_SQUARE\")).as_(\"MED_PRICE\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ADS_df_GrpbyCodeInsee_MedianPrice.limit(10).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-locking",
   "metadata": {},
   "source": [
    "### Removing outliers\n",
    "We can join our median to our listing data and filter out any listing that deviates too far from the mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADS_df_joinMedian = ADS_df_Raw.join(\n",
    "  ADS_df_GrpbyCodeInsee_MedianPrice,\n",
    "  (ADS_df_Raw.col(\"ADS_GEO_ZIPCODE\") == ADS_df_GrpbyCodeInsee_MedianPrice.col(\"ADS_GEO_ZIPCODE\"))\n",
    "  &(ADS_df_Raw.col(\"ADS_GEO_CITY\") == ADS_df_GrpbyCodeInsee_MedianPrice.col(\"ADS_GEO_CITY\"))\n",
    "  &(ADS_df_Raw.col(\"ADS_ATTR_REAL_ESTATE_TYPE\") == ADS_df_GrpbyCodeInsee_MedianPrice.col(\"ADS_ATTR_REAL_ESTATE_TYPE_MEDIAN\")))\n",
    "\n",
    "ADS_df_Clean = ADS_df_joinMedian.withColumn(\"ADS_PRICE_SQUARE\", col(\"ADS_PRICE\") / col(\"ADS_ATTR_SQUARE\")).filter(\n",
    "  (col(\"ADS_PRICE_SQUARE\") / (col(\"ADS_PRICE_SQUARE\") + col(\"MED_PRICE\")) >= 0.25)\n",
    "  & (col(\"ADS_PRICE_SQUARE\") / (col(\"ADS_PRICE_SQUARE\") + col(\"MED_PRICE\")) < 0.75)\n",
    "  & (col(\"ADS_PRICE_SQUARE\") < 150)\n",
    "  & (col(\"ADS_PRICE_SQUARE\") > 0)\n",
    "  & (col(\"ADS_ATTR_SQUARE\") >= 9)\n",
    "  & (col(\"ADS_ATTR_SQUARE\") <= 300)\n",
    "  & (col(\"COUNT_ADS\") >= 5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ADS_df_Clean.limit(10).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-direction",
   "metadata": {},
   "source": [
    "### Cleaning the data for data science\n",
    "While having listing types in readable words is useful for the business, it is not ideal for our model training, we create a couple of mapping functions to transform our words to numbers!\n",
    "\n",
    "Snowpark will seamlessly push these functions to snowflake as Python UDFs.\n",
    "\n",
    "**Note:** We would have had to write a SQL CASE statement, but instead we wrote a Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(name=\"ADS_ATTR_FURNISHED_Encode_Python\", is_permanent=True, stage_location=\"@SnowParkDemo_Stage\", replace=True)\n",
    "def ADS_ATTR_FURNISHED_Encode_Python(x : str) -> int:\n",
    "  if x == \"Meublé\":\n",
    "    return 2\n",
    "  elif x == \"Non meublé\":\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(name=\"ADS_ATTR_REAL_ESTATE_TYPE_Encode_Python\", is_permanent=True, stage_location=\"@SnowParkDemo_Stage\", replace=True)\n",
    "def ADS_ATTR_REAL_ESTATE_TYPE_Encode_Python(x : str) -> int:\n",
    "  if x == \"Maison\":\n",
    "    return 2\n",
    "  elif x == \"Appartement\":\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-hungarian",
   "metadata": {},
   "source": [
    "Now that these functions are available in snowflake as UDFs we can simply call them from our dataframe to clean our result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-leonard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ADS_df_final = ADS_df_Clean.select(\n",
    "        col('ADS_GEO_LAT')\n",
    "        ,col('ADS_GEO_LNG')\n",
    "        ,col('ADS_ATTR_ROOMS')\n",
    "        ,col('ADS_ATTR_SQUARE')\n",
    "        ,ADS_ATTR_REAL_ESTATE_TYPE_Encode_Python(col(\"ADS_ATTR_REAL_ESTATE_TYPE\")).as_('ADS_ATTR_REAL_ESTATE_TYPE_NUM')\n",
    "        ,ADS_ATTR_FURNISHED_Encode_Python(col(\"ADS_ATTR_FURNISHED\")).as_('ADS_ATTR_FURNISHED_NUM')\n",
    "        ,col('ADS_PRICE'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-payment",
   "metadata": {},
   "source": [
    "We can get the execution plan and SQL needed to prefrom all our steps by calling explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADS_df_final.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ADS_df_final.limit(10).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-fellowship",
   "metadata": {},
   "source": [
    "# Get the prepared data and train the XGBoost Model\n",
    "We cast the retrieve data as dataframe, split these data in training / test dataframe using sklearn and cast them in DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test / Train\n",
    "CollectedDataframe = pd.DataFrame(ADS_df_final.collect())\n",
    "target = \"ADS_PRICE\"\n",
    "predictors = [x for x in CollectedDataframe.columns if x not in [target]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(CollectedDataframe[predictors] , CollectedDataframe[target], test_size=0.1)\n",
    "\n",
    "#Create DMatrix\n",
    "import xgboost as xgb\n",
    "DMatrix_train = xgb.DMatrix(X_train, label=y_train)\n",
    "DMatrix_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "print(\"DMatrix_train (\", DMatrix_train.num_row() ,\", \",DMatrix_train.num_col(),\")\")\n",
    "print(\"DMatrix_test  (\", DMatrix_test.num_row()  ,\", \",DMatrix_test.num_col(),\")\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-nickname",
   "metadata": {},
   "source": [
    "We setup the XGBoost parameters for the training and run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-curve",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param={}\n",
    "param['booster']='gbtree'\n",
    "param['objective']= 'reg:squarederror'\n",
    "param['eta']=0.05\n",
    "param['max_depth']=10\n",
    "param['min_child_weight']=1\n",
    "param['gamma']=1\n",
    "param['subsample']=0.75\n",
    "param['colsample_bytree']=0.75\n",
    "param['scale_pos_weight']=1\n",
    "param['nthread'] = -1\n",
    "param['verbosity'] = 1\n",
    "\n",
    "\n",
    "evallist = [(DMatrix_train, 'train'), (DMatrix_test, 'eval')]\n",
    "num_round=100\n",
    "bst = xgb.train(param, DMatrix_train, num_round, evallist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-remove",
   "metadata": {},
   "source": [
    "Print the features importance and the variance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-costume",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "xgb.plot_importance(bst)\n",
    "\n",
    "# make predictions for test data\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "pred_test = bst.predict(DMatrix_test)\n",
    "print(\"pred_test : \", explained_variance_score(pred_test,DMatrix_test.get_label()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-charger",
   "metadata": {},
   "source": [
    "# Creating the Python Inference function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-allergy",
   "metadata": {},
   "source": [
    "### Register the Python Inference Function as Python UDF on Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySnowSess.add_packages(\"xgboost\",\"pandas\")\n",
    "\n",
    "@udf(name=\"get_XGBoost_RENT_PRICE\", is_permanent=True, stage_location=\"@SnowParkDemo_Stage\", replace=True)\n",
    "def get_XGBoost_RENT_PRICE(lat: float, lng: float, rooms: int, square: float, realestatetype: int, furnished: int) -> float:\n",
    "    import pandas\n",
    "    import xgboost as xgb\n",
    "    RowDataFrame = pandas.DataFrame([[lat, lng, rooms, square, realestatetype, furnished]],columns=['ADS_GEO_LAT', 'ADS_GEO_LNG', 'ADS_ATTR_ROOMS', 'ADS_ATTR_SQUARE', 'ADS_ATTR_REAL_ESTATE_TYPE_NUM', 'ADS_ATTR_FURNISHED_NUM'])\n",
    "    RowDMatrix = xgb.DMatrix(RowDataFrame)\n",
    "    return bst.predict(RowDMatrix)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-warner",
   "metadata": {},
   "source": [
    "# Using our UDF Function to run our model on Snowflake "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-clock",
   "metadata": {},
   "source": [
    "### Classifying our entire data and writing it to a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySnowSess.table(\"ADS\").filter((col(\"ADS_CATEGORY_NAME\") ==  \"Ventes immobilières\" )). \\\n",
    "    withColumn(\"EstimatedRentPrice\",get_XGBoost_RENT_PRICE(\n",
    "        col(\"ADS_GEO_LAT\"),\n",
    "        col(\"ADS_GEO_LNG\"),\n",
    "        col(\"ADS_ATTR_ROOMS\"),\n",
    "        col(\"ADS_ATTR_SQUARE\"),\n",
    "        ADS_ATTR_REAL_ESTATE_TYPE_Encode_Python(col(\"ADS_ATTR_REAL_ESTATE_TYPE\")),\n",
    "        ADS_ATTR_FURNISHED_Encode_Python(col(\"ADS_ATTR_FURNISHED\")))).\\\n",
    "    write.mode(\"overwrite\").saveAsTable(\"ADS_RENT_PRED_XGBOOST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-circular",
   "metadata": {},
   "source": [
    "### Alternatively, use our Classifier on demand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mySnowSess.table(\"ADS\").filter((col(\"ADS_GEO_ZIPCODE\") ==  \"92400\" ) & (col(\"ADS_CATEGORY_NAME\") ==  \"Ventes immobilières\" )). \\\n",
    "    withColumn(\"EstimatedRentPrice\",get_XGBoost_RENT_PRICE(\n",
    "        col(\"ADS_GEO_LAT\"),\n",
    "        col(\"ADS_GEO_LNG\"),\n",
    "        col(\"ADS_ATTR_ROOMS\"),\n",
    "        col(\"ADS_ATTR_SQUARE\"),\n",
    "        ADS_ATTR_REAL_ESTATE_TYPE_Encode_Python(col(\"ADS_ATTR_REAL_ESTATE_TYPE\")),\n",
    "        ADS_ATTR_FURNISHED_Encode_Python(col(\"ADS_ATTR_FURNISHED\")))). \\\n",
    "select(col(\"ADS_SUBJECT\"),col(\"ADS_GEO_CITY\"),col(\"EstimatedRentPrice\"),col(\"ADS_PRICE\")).limit(10).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-excuse",
   "metadata": {},
   "source": [
    "### Optional :  We can also write the model to file and push it on Snowflake for A/B Testing UDF\n",
    "\n",
    "```python\n",
    "create or replace function GetLocPriceXGBoostABTest(model VARCHAR, lat  FLOAT, lng  FLOAT, rooms  NUMBER, square FLOAT, realestatetype  NUMBER, furnished  NUMBER)\n",
    "returns FLOAT\n",
    "language python\n",
    "runtime_version = '3.8'\n",
    "packages = ('xgboost', 'pandas')\n",
    "imports=('@snowparkdemo_stage/AnnoncePriceLocations_A.xbmodel','@snowparkdemo_stage/AnnoncePriceLocations_B.xbmodel')\n",
    "handler = 'GetLocPriceXGBoost'\n",
    "as\n",
    "$$\n",
    "import sys\n",
    "IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "def GetLocPriceXGBoost(model: str, lat: float, lng: float, rooms: int, square: float, realestatetype: int, furnished: int):\n",
    "    import pandas\n",
    "    import xgboost as xgb\n",
    "    file_path = import_dir + \"AnnoncePriceLocations_\" + model + \".xbmodel\"\n",
    "    bst = xgb.Booster({'nthread': 1})  # init model\n",
    "    bst.load_model(file_path)\n",
    "    RowDataFrame = pandas.DataFrame([[lat, lng, rooms, square, realestatetype, furnished]],columns=['ADS_GEO_LAT', 'ADS_GEO_LNG', 'ADS_ATTR_ROOMS', 'ADS_ATTR_SQUARE', 'ADS_ATTR_REAL_ESTATE_TYPE_NUM', 'ADS_ATTR_FURNISHED_NUM'])\n",
    "    RowDMatrix = xgb.DMatrix(RowDataFrame)\n",
    "    return bst.predict(RowDMatrix)[0]\n",
    "$$;\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bst.save_model(\"./AnnoncePriceLocations_A.xbmodel\")\n",
    "#mySnowSess.sql(\"put file:///Users/apicard/Documents/SnowFlake/Project/SnowParkPython/Project_1/SnowparkDemo/snowpark-python-xgboost-realestate/AnnoncePriceLocations_A.xbmodel @SnowParkDemo_Stage overwrite=true\").collect()\n",
    "\n",
    "#bst.save_model(\"./AnnoncePriceLocations_B.xbmodel\")\n",
    "#mySnowSess.sql(\"put file:///Users/apicard/Documents/SnowFlake/Project/SnowParkPython/Project_1/SnowparkDemo/snowpark-python-xgboost-realestate/AnnoncePriceLocations_B.xbmodel @SnowParkDemo_Stage overwrite=true\").collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySnowSess.close();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
